import os
import time
import re
from typing import List, Optional, Tuple
import numpy as np
from fastapi import FastAPI, Body
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from pydantic import BaseModel

# --- AI Model Loading ---
EMB_MODEL = os.getenv("EMB_MODEL", "intfloat/multilingual-e5-small")
_embedder = None
try:
    from sentence_transformers import SentenceTransformer
    print(f"Loading embedding model: {EMB_MODEL}...")
    _embedder = SentenceTransformer(EMB_MODEL)
    print("Model loaded successfully.")
except Exception as e:
    print(f"Warning: Failed to load sentence-transformer model: {e}")
    print("Running in regex-only mode.")
    _embedder = None
# --- End of AI Model Loading ---

# =========================
# Pydantic Schemas
# =========================
class Source(BaseModel):
    doc: str
    page: Optional[int] = None
    quote: Optional[str] = None


class Hit(BaseModel):
    span: str
    label: str
    replacement: Optional[str] = None
    confidence: float
    source: Source
    start: int
    end: int


class AnalyzeRequest(BaseModel):
    text: str
    policy_version: str = "2024-03"


class AnalyzeResponse(BaseModel):
    hits: List[Hit]
    latency_ms: int


# =========================
# FastAPI App Setup
# =========================
app = FastAPI(title="Seongnam LifeRec Checker", version="1.6.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=False,
    allow_methods=["*"], allow_headers=["*"],
)

# =========================
# Policy Rules Database
# =========================
RULES = [
    # --- ìƒí˜¸ëª…, í”Œë«í¼ëª… ---
    {"pattern": r"(?:NAVER|Naver|ë„¤ì´ë²„|Daum|ë‹¤ìŒ|\bGoogle\b(?!\s?(Docs|Classroom|TV)))", "label": "ìƒí˜¸ëª…", "replacement": "í¬í„¸ì‚¬ì´íŠ¸", "confidence": 0.95, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "Google(êµ¬ê¸€), NAVER(ë„¤ì´ë²„), Daum(ë‹¤ìŒ) ë“± â†’ í¬í„¸ì‚¬ì´íŠ¸"}, "aliases": ["googel", "gooogle", "êµ¬ê¸€ê²€ìƒ‰", "ë„¤ì´ë²„ê²€ìƒ‰", "ë‹¤ìŒê²€ìƒ‰"]},
    {"pattern": r"(?:Google\s?Classroom|êµ¬ê¸€\s?í´ë˜ìŠ¤ë£¸|EBS\s?ì˜¨ë¼ì¸í´ë˜ìŠ¤|classting|í´ë˜ìŠ¤íŒ…)", "label": "ìƒí˜¸ëª…", "replacement": "í•™ìŠµ í”Œë«í¼", "confidence": 0.95, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "Google Classroom(êµ¬ê¸€ í´ë˜ìŠ¤ë£¸), EBS ì˜¨ë¼ì¸í´ë˜ìŠ¤ ë“± â†’ í•™ìŠµ í”Œë«í¼"}, "aliases": ["gclassroom", "êµ¬í´", "í´ë˜ìŠ¤íŒ…ì•±", "ì´ë¹„ì—ìŠ¤ ì˜¨ë¼ì¸í´ë˜ìŠ¤"]},
    {"pattern": r"(?:TikTok|í‹±í†¡)", "label": "ìƒí˜¸ëª…", "replacement": "ì—”í„°í…Œì¸ë¨¼íŠ¸ í”Œë«í¼", "confidence": 0.92, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "TikTok(í‹±í†¡) ë“± â†’ ì—”í„°í…Œì¸ë¨¼íŠ¸ í”Œë«í¼"}, "aliases": ["í‹±í†¡ì˜ìƒ", "tiktoc", "í‹±í†¡ìŠ¤"]},
    {"pattern": r"(?:YouTube|ìœ íŠœë¸Œ|TVING|í‹°ë¹™|watcha|ì™“ì± |netflix|ë„·í”Œë¦­ìŠ¤|wavve|ì›¨ì´ë¸Œ|disney\s?plus|ë””ì¦ˆë‹ˆ\+?|ë””ì¦ˆë‹ˆí”ŒëŸ¬ìŠ¤|OTT)", "label": "ìƒí˜¸ëª…", "replacement": "ë™ì˜ìƒ í”Œë«í¼", "confidence": 0.95, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "YouTube(ìœ íŠœë¸Œ), TVING(í‹°ë¹™) ... OTT ë“± â†’ ë™ì˜ìƒ í”Œë«í¼"}, "aliases": ["yutube", "you tube", "ìœ íŠ­", "ìœ íˆ½", "ë„·í”Œ", "ì™“ì± í”Œë ˆì´"]},
    {"pattern": r"(?:YouTuber|ìœ íŠœë²„)", "label": "ì§ì—…ëª…", "replacement": "ë™ì˜ìƒ í¬ë¦¬ì—ì´í„°", "confidence": 0.92, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "YouTuber(ìœ íŠœë²„) ë“± â†’ ë™ì˜ìƒ í¬ë¦¬ì—ì´í„°, ë™ì˜ìƒ ì œê³µì"}, "aliases": ["ìœ íŠœë¸ŒëŸ¬", "youtuber"]},
    {"pattern": r"(?:KakaoTalk|ì¹´ì¹´ì˜¤í†¡|ì¹´í†¡|LINE|ë¼ì¸|Instagram|ì¸ìŠ¤íƒ€ê·¸ë¨|Twitter|íŠ¸ìœ„í„°|Meta|ë©”íƒ€|Facebook|í˜ì´ìŠ¤ë¶)", "label": "ìƒí˜¸ëª…", "replacement": "ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ì„œë¹„ìŠ¤", "confidence": 0.95, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "KakaoTalk, Instagram, Facebook ë“± â†’ ë©”ì‹ ì €, ì†Œì…œë„¤íŠ¸ì›Œí¬ì„œë¹„ìŠ¤"}, "aliases": ["kakaotalk", "kkt", "ì¹´í†¡ë°©", "ì¸ìŠ¤íƒ€", "insta", "í˜ë¶", "x(íŠ¸ìœ„í„°)"]},
    {"pattern": r"(?:Chat\s?GPT|ì±—\s?GPT|ì±—ì§€í”¼í‹°|wrtn|ë¤¼íŠ¼|bing\s?Chat|ë¹™ì±—|Bard|ë°”ë“œ|í•˜ì´í¼í´ë¡œë°”X|HyperClova\s?X)", "label": "ìƒí˜¸ëª…", "replacement": "ìƒì„±í˜• ì¸ê³µì§€ëŠ¥", "confidence": 0.95, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "Chat GPT(ì±—ì§€í”¼í‹°), wrtn(ë¤¼íŠ¼) ... ë“± â†’ ëŒ€í™”í˜• ì¸ê³µì§€ëŠ¥, ìƒì„±í˜• ì¸ê³µì§€ëŠ¥"}, "aliases": ["chatgpt", "ì±—ì¥í”¼í‹°", "gptì±—", "ë¹™ì±—ë´‡", "í•˜í´x", "ë¤¼íŠ¼ai"]},
    {"pattern": r"(?:Canva|ìº”ë°”|miricanvas|ë¯¸ë¦¬ìº”ë²„ìŠ¤|mangoboard|ë§ê³ ë³´ë“œ)", "label": "ìƒí˜¸ëª…", "replacement": "ë””ìì¸ ì œì‘ í”Œë«í¼", "confidence": 0.92, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "miricanvas(ë¯¸ë¦¬ìº”ë²„ìŠ¤), mangoboard(ë§ê³ ë³´ë“œ), Canva(ìº”ë°”) ë“±"}, "aliases": ["ìº”ë°”ì•±", "ë¯¸ìº”"]},
    {"pattern": r"(?:KineMaster|í‚¤ë„¤ë§ˆìŠ¤í„°|Premiere\s?Pro|í”„ë¦¬ë¯¸ì–´\s?í”„ë¡œ)", "label": "í”„ë¡œê·¸ë¨ëª…", "replacement": "ì˜ìƒ í¸ì§‘ í”„ë¡œê·¸ë¨", "confidence": 0.92, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "ì˜ìƒ ì œì‘ í”„ë¡œê·¸ë¨, ì˜ìƒ í¸ì§‘ í”„ë¡œê·¸ë¨"}, "aliases": ["í‚¤ë„¤", "í”„ë¦¬ë¯¸ì–´"]},
    # --- ê°œë°œ ì–¸ì–´ / ê°œë°œ ë„êµ¬ ---
    {"pattern": r"(?:Python|íŒŒì´ì¬|Java\b|ìë°”|C\+\+|Cì–¸ì–´|ìë°”\s?ìŠ¤í¬ë¦½íŠ¸|Java\s*Script|JavaScript|Javascript|JS\b)", "label": "í”„ë¡œê·¸ë¨ëª…", "replacement": "í”„ë¡œê·¸ë˜ë° ì–¸ì–´", "confidence": 0.92, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 2, "quote": "í”„ë¡œê·¸ë¨ëª… (íŒŒì´ì¬, Cì–¸ì–´ ë“±) ê¸°ì¬ ë¶ˆê°€"}, "aliases": ["íŒŒì´ì„ ", "ìë°”ìŠ¤í¬ë¦½", "ì”¨ì–¸ì–´", "javascript", "java script", "js"]},
    {"pattern": r"(?:Jupyter|ì£¼í”¼í„°|Colab|ì½”ë©|PyCharm|íŒŒì´ì°¸|VS\s?Code|Visual\s?Studio\s?Code|ë¹„ì£¼ì–¼\s?ìŠ¤íŠœë””ì˜¤\s?ì½”ë“œ|Anaconda|Spyder)", "label": "í”„ë¡œê·¸ë¨ëª…", "replacement": "ê°œë°œ ë„êµ¬", "confidence": 0.92, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 2, "quote": "íŠ¹ì • ì†Œí”„íŠ¸ì›¨ì–´/ê°œë°œí™˜ê²½ ê¸°ì¬ ì§€ì–‘, ì¼ë°˜í™” í‘œí˜„ ì‚¬ìš©"}, "aliases": ["ì£¼í”¼í„°", "ì½”ë©", "íŒŒì´ì°¸", "vscode", "vsì½”ë“œ"]},
    # --- ì˜¤í”¼ìŠ¤ / ë¬¸ì„œ ---
    {"pattern": r"(?:MS\s?ì›Œë“œ|MS\s?Word|Microsoft\s?Word|ì›Œë“œ)", "label": "í”„ë¡œê·¸ë¨ëª…", "replacement": "ë¬¸ì„œì‘ì„± í”„ë¡œê·¸ë¨", "confidence": 0.92, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "hwp, MSì›Œë“œ â†’ ë¬¸ì„œì‘ì„± í”„ë¡œê·¸ë¨"}, "aliases": ["msword", "ì›Œë“œíŒŒì¼"]},
    {"pattern": r"(?:Google\s?Docs|êµ¬ê¸€\s?ë¬¸ì„œ|êµ¬ê¸€\s?ë…ìŠ¤)", "label": "í”„ë¡œê·¸ë¨ëª…", "replacement": "ì˜¨ë¼ì¸ ë¬¸ì„œ í¸ì§‘ê¸°", "confidence": 0.92, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "Google Docs(êµ¬ê¸€ë¬¸ì„œ) ë“± â†’ ì˜¨ë¼ì¸ ë¬¸ì„œ í¸ì§‘ê¸°"}, "aliases": ["gdocs"]},
    # --- ê°•ì—° / ì´ë™ìˆ˜ë‹¨ / í™”ìƒíšŒì˜ ---
    {"pattern": r"(?:TED|í…Œë“œ)", "label": "ê°•ì—°ëª…", "replacement": "ì˜¨ë¼ì¸ ê°•ì—°íšŒ", "confidence": 0.92, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "TED(í…Œë“œ) ë“± â†’ ì˜¨ë¼ì¸ ê°•ì—°íšŒ"}, "aliases": ["ted ê°•ì—°", "í…Œë“œí†¡"]},
    {"pattern": r"(?:KTX|ì¼€ì´í‹°ì—‘ìŠ¤|SRT|ì—ìŠ¤ì•Œí‹°)", "label": "ìƒí˜¸ëª…", "replacement": "ê³ ì† ì—´ì°¨", "confidence": 0.95, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "KTX, SRT â†’ ê³ ì† ì—´ì°¨"}, "aliases": ["ì¼€í…", "ì—ìŠ¤ì•Œí‹°"]},
    {"pattern": r"(?:Zoom|ì¤Œ|ì›¨ì¼ì˜¨|Whale\s?ON)", "label": "ìƒí˜¸ëª…", "replacement": "í™”ìƒ íšŒì˜", "confidence": 0.92, "source": {"doc": "ëŒ€ì²´í‘œí˜„", "page": 1, "quote": "Zoom(ì¤Œ) ë“± â†’ í™”ìƒ íšŒì˜"}, "aliases": ["ì¤Œë¯¸íŒ…", "ì›¨ì¼ì˜¨íšŒì˜"]},
    # --- ê¸°ê´€ëª… / ë…¼ë¬¸ / ì™¸êµ­ì–´ / íŠ¹ìˆ˜ë¬¸ì ---
    {"pattern": r"(?:UN|EU|WHO|WTO|OECD|IMF|UNESCO|IAEA|NATO|ìœ ì—”|ìœ ëŸ½ì—°í•©|ì„¸ê³„\s?ë³´ê±´\s?ê¸°êµ¬|ì„¸ê³„\s?ë¬´ì—­\s?ê¸°êµ¬|ê²½ì œí˜‘ë ¥ê°œë°œê¸°êµ¬|êµ­ì œí†µí™”ê¸°ê¸ˆ|ìœ ë„¤ìŠ¤ì½”|êµ­ì œì›ìë ¥ê¸°êµ¬|ë¶ëŒ€ì„œì–‘ì¡°ì•½ê¸°êµ¬)", "label": "ê¸°ê´€ëª…", "replacement": "êµ­ì œê¸°êµ¬", "confidence": 0.98, "source": {"doc": "ë‹¨ì²´ëª… ê¸°ì¬", "page": 1, "quote": "êµìœ¡ê´€ë ¨ê¸°ê´€ ì œì™¸ íŠ¹ì • ê¸°ê´€ëª… ê¸°ì¬ ë¶ˆê°€"}, "aliases": ["ìœ ì—”ê¸°êµ¬", "ì˜¤ì´ì‹œë””", "ë‚˜í† ", "ìœ ë„¤ìŠ¤ì½”í•œêµ­ìœ„ì›íšŒ"]},
    {"pattern": r"ì†Œë…¼ë¬¸|ì—°êµ¬ë³´ê³ ì„œ", "label": "ë…¼ë¬¸ ì‹¤ì ", "replacement": "íƒêµ¬ í™œë™", "confidence": 0.99, "source": {"doc": "ë…¼ë¬¸ ê¸°ì¬", "page": 1, "quote": "ììœ¨íƒêµ¬í™œë™ ì‚°ì¶œë¬¼ ì‹¤ì  ê¸°ì¬ ë¶ˆê°€"}, "aliases": ["ì†Œë…¼ë¬¸ ì‘ì„±", "ì—°êµ¬ë³´ê³ ì„œë¥¼ ì œì¶œ"]},
    {"pattern": r"[ä¸€-é¾¥]", "label": "ì™¸êµ­ì–´", "replacement": None, "confidence": 0.99, "source": {"doc": "ì™¸êµ­ì–´ ê¸°ì¬", "page": 1, "quote": "í•œê¸€ ì‚¬ìš© ì›ì¹™. ì˜ë¬¸ ì œì™¸ ì™¸êµ­ì–´ ì…ë ¥ ë¶ˆê°€."}, "aliases": []},
    {"pattern": r"[Â·â€»â–·â–¶]", "label": "íŠ¹ìˆ˜ë¬¸ì", "replacement": " ", "confidence": 0.99, "source": {"doc": "íŠ¹ìˆ˜ë¬¸ì", "page": 1, "quote": "ì„œìˆ í˜• íŠ¹ìˆ˜ë¬¸ì ì…ë ¥ ì§€ì–‘"}, "aliases": []},
    # --- í•™ìˆ  ìš©ì–´(ì¼ë°˜í™”) ---
    {"pattern": r"(?:CRISPR-?Cas9|í¬ë¦¬ìŠ¤í¼-?ì¹´ìŠ¤9?)", "label": "ì „ë¬¸ ìš©ì–´", "replacement": "ìœ ì „ì ê°€ìœ„ ê¸°ìˆ ", "confidence": 0.93, "source": {"doc": "í•™ìˆ  ìš©ì–´ ì¼ë°˜í™”", "page": 1, "quote": "ê³¼ë„í•œ ì „ë¬¸ìš©ì–´ëŠ” ì¼ë°˜í™”/ì„¤ëª…ì  í‘œí˜„ ì‚¬ìš© ê¶Œì¥"}, "aliases": ["crispr", "cas9", "í¬ë¦¬ìŠ¤í¼"]},
]

# =========================
# Core AI Engine Logic + Heuristics
# =========================

# --- Preview/Apply thresholds ---
MIN_PREVIEW_CONF = float(os.getenv("MIN_PREVIEW_CONF", "0.90"))

# --- Korean token heuristics (to tame semantic) ---
_JOSA_RE = re.compile(r"(ìœ¼ë¡œ|ë¼ì„œ|ë¼ë©°|ë¼ê³ |ì´ë¼|ë¼|ì„|ë¥¼|ì€|ëŠ”|ì´|ê°€|ì—|ì—ì„œ|ì—ê²Œ|ê»˜ì„œ|ë¡œ|ì™€|ê³¼|ë„|ë§Œ|ê¹Œì§€|ë¶€í„°|ì²˜ëŸ¼|ë³´ë‹¤|ê»˜|í•œí…Œ|ì—ê²Œì„œ|ì´ë‹¤|í•¨)$")
STOPWORDS_KO = {
    "í”„ë¡œê·¸ë¨", "ê°œë°œ", "ì§„í–‰", "í†µí•´", "ì‘ì„±", "ì •ë¦¬", "ì œì¶œ", "ë³´ê³ ì„œ",
    "ë°œí‘œ", "ì´ë™", "ì œì‘", "í¸ì§‘", "ë§ˆë¬´ë¦¬", "í™œìš©", "ì°¸ì¡°", "ì˜®ê²¼ë‹¤",
    "ì´ˆì•ˆ", "ì •ë¦¬í•˜ê³ ", "ì˜®ê¹€", "ê²€í† ", "ê²°ê³¼", "ë‚´ìš©", "í•™ìŠµ", "í™œë™"
}


def _normalize_ko_token(tok: str) -> str:
    if re.fullmatch(r"[ê°€-í£]+", tok):
        return _JOSA_RE.sub("", tok)
    return tok


def _should_consider_token(tok: str) -> bool:
    base = _normalize_ko_token(tok)
    if base in STOPWORDS_KO:
        return False
    if re.fullmatch(r"[ê°€-í£]+", tok):
        if not re.search(r"(í†¡|ê·¸ë¨|ë¶|ë„·í”Œ|ì™“ì± |ì›¨ì¼ì˜¨|ìœ íŠ­|ìœ íŠœë¸Œ|í‹°ë¹™|ìº”ë°”|í‚¤ë„¤|í‹±í†¡|í˜ë¶|ì¸ìŠ¤íƒ€|í´ë˜ìŠ¤ë£¸|ì½”ë©|ì£¼í”¼í„°|íŒŒì´ì°¸)$", base):
            return False
    return True


# --- Build embedding alias index (for fuzzy) ---
def _build_alias_index():
    if _embedder is None:
        return None, None
    alias_rules, alias_texts = [], []
    for rule in RULES:
        for alias in rule.get("aliases", []):
            if text := alias.strip():
                alias_texts.append(text)
                alias_rules.append(rule)
    if not alias_texts:
        return None, None
    print(f"Building embedding index for {len(alias_texts)} aliases...")
    alias_embeddings = _embedder.encode(alias_texts, normalize_embeddings=True, show_progress_bar=False)
    print("Embedding index built successfully.")
    return np.array(alias_embeddings, dtype=np.float32), alias_rules


_ALIAS_EMB_INDEX, _ALIAS_RULES = _build_alias_index()

# --- Build exact alias map (for safe auto-replace of common typos like 'yutube') ---
_ALIAS_EXACT_MAP = {}
for rule in RULES:
    for a in rule.get("aliases", []):
        if not a:
            continue
        _ALIAS_EXACT_MAP[a.lower()] = rule


def regex_match(text: str) -> List[Hit]:
    hits: List[Hit] = []
    for rule in RULES:
        for match in re.finditer(rule["pattern"], text, flags=re.IGNORECASE):
            src = rule["source"]
            hits.append(Hit(
                span=match.group(0), label=rule["label"], replacement=rule.get("replacement"),
                confidence=float(rule.get("confidence", 0.9)),
                source=Source(doc=src.get("doc", ""), page=src.get("page"), quote=src.get("quote", "")),
                start=match.start(), end=match.end()
            ))
    return hits


def alias_exact_match(text: str) -> List[Hit]:
    """Exact alias hits (auto-replace ok): e.g., 'yutube' -> YouTube category."""
    hits: List[Hit] = []
    if not _ALIAS_EXACT_MAP:
        return hits
    aliases = sorted(_ALIAS_EXACT_MAP.keys(), key=len, reverse=True)
    alt = "|".join(re.escape(a) for a in aliases)
    pattern = re.compile(rf"(?<![A-Za-z0-9ê°€-í£])({alt})(?![A-Za-z0-9ê°€-í£])", re.IGNORECASE)
    for m in pattern.finditer(text):
        key = m.group(1).lower()
        rule = _ALIAS_EXACT_MAP.get(key)
        if not rule:
            continue
        src = rule["source"]
        hits.append(Hit(
            span=m.group(0), label=rule["label"], replacement=rule.get("replacement"),
            confidence=0.94,
            source=Source(doc=src.get("doc", ""), page=src.get("page"), quote=src.get("quote", "")),
            start=m.start(), end=m.end()
        ))
    return hits


def _get_semantic_candidates(text: str, min_len=2, max_len=30):
    for match in re.finditer(r"\b[A-Za-zê°€-í£0-9][A-Za-zê°€-í£0-9\.\-_/()]*\b", text):
        token = match.group(0).strip()
        if not (min_len <= len(token) <= max_len):
            continue
        if not _should_consider_token(token):
            continue
        yield token, match.start(), match.end()


def semantic_match(text: str, threshold: float = 0.86, max_hits: int = 10) -> List[Hit]:
    if not (_embedder and _ALIAS_EMB_INDEX is not None):
        return []
    candidates = list(_get_semantic_candidates(text))
    if not candidates:
        return []
    cand_tokens = [c[0] for c in candidates]
    cand_embeddings = _embedder.encode(cand_tokens, normalize_embeddings=True, show_progress_bar=False)
    sim_matrix = np.matmul(np.array(cand_embeddings, dtype=np.float32), _ALIAS_EMB_INDEX.T)
    hits: List[Hit] = []
    used_spans = set()
    for i, row in enumerate(sim_matrix):
        if len(hits) >= max_hits:
            break
        best_idx = int(np.argmax(row))
        score = float(row[best_idx])
        second = float(np.partition(row, -2)[-2]) if row.size > 1 else 0.0
        if not (score >= threshold and (score - second) >= 0.06):
            continue
        span_text, start, end = candidates[i]
        if (start, end) in used_spans:
            continue
        matched_rule = _ALIAS_RULES[best_idx]
        conf = min(0.88, max(0.6, score * 0.85))  # < 0.90 â†’ preview highlight only
        hits.append(Hit(
            span=span_text, label=matched_rule["label"], replacement=matched_rule.get("replacement"),
            confidence=float(conf), source=Source(**matched_rule["source"]),
            start=start, end=end
        ))
        used_spans.add((start, end))
    return hits


def collapse_parenthetical_duplicates(text: str, hits: List[Hit]) -> List[Hit]:
    """
    Collapse patterns like 'ìœ ì—”(UN)' â†’ one replacement ('êµ­ì œê¸°êµ¬'),
    instead of replacing both 'ìœ ì—”' and 'UN' â†’ 'êµ­ì œê¸°êµ¬(êµ­ì œê¸°êµ¬)'.
    Works when both inner and outer are hits with the same label & replacement.
    """
    hits = sorted(hits, key=lambda h: (h.start, h.end))
    n = len(hits)
    used = [False] * n
    result: List[Hit] = []
    i = 0
    while i < n:
        if used[i]:
            i += 1
            continue
        hi = hits[i]
        # look for "( ... )" right after hi
        k = hi.end
        # skip spaces
        while k < len(text) and text[k].isspace():
            k += 1
        if k < len(text) and text[k] == "(":
            # find a next hit wholly inside the parentheses
            j = i + 1
            while j < n and hits[j].start < k + 1:
                j += 1
            if j < n:
                hj = hits[j]
                # skip spaces after '('
                inner_start = k + 1
                while inner_start < len(text) and text[inner_start].isspace():
                    inner_start += 1
                # find closing ')'
                # allow spaces before ')'
                inner_end = hj.end
                tmp = inner_end
                while tmp < len(text) and text[tmp].isspace():
                    tmp += 1
                if tmp < len(text) and text[tmp] == ")":
                    close_pos = tmp
                    # verify both hits share same label+replacement and hj is inside the parens
                    if (hj.start >= inner_start) and (hj.end <= close_pos) and \
                       (hi.label == hj.label) and (hi.replacement == hj.replacement) and \
                       (hi.replacement is not None):
                        # create combined hit
                        combined = Hit(
                            span=text[hi.start:close_pos + 1],
                            label=hi.label,
                            replacement=hi.replacement,
                            confidence=max(hi.confidence, hj.confidence),
                            source=hi.source,
                            start=hi.start,
                            end=close_pos + 1
                        )
                        result.append(combined)
                        used[i] = True
                        used[j] = True
                        i += 1
                        continue
        # default path: keep hi
        result.append(hi)
        used[i] = True
        i += 1
    # keep any not-yet-added hits
    for idx in range(n):
        if not used[idx]:
            result.append(hits[idx])
    # de-duplicate and sort
    seen = set()
    dedup: List[Hit] = []
    for h in sorted(result, key=lambda h: (h.start, h.end)):
        key = (h.start, h.end, h.label, h.replacement)
        if key in seen:
            continue
        seen.add(key)
        dedup.append(h)
    return dedup


def merge_hits(*hit_groups: List[Hit]) -> List[Hit]:
    all_hits: List[Hit] = []
    for g in hit_groups:
        all_hits.extend(g)
    all_hits = sorted(all_hits, key=lambda h: (h.start, -(h.end - h.start), -h.confidence))
    merged: List[Hit] = []
    if not all_hits:
        return []
    last_hit_end = -1
    for hit in all_hits:
        if hit.start >= last_hit_end:
            merged.append(hit)
            last_hit_end = hit.end
    return merged


# =========================
# UI (HTML/CSS/JS)
# =========================
HTML_PAGE = """
<!doctype html><html lang="ko"><head>
<meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>ìƒí™œê¸°ë¡ë¶€ ìë™ ì ê²€ â€“ ë°ëª¨ v1.6</title>
<style>:root{--bg:#0b1020;--card:#111830;--ink:#e6edff;--muted:#9db1ff;--accent:#4f7cff;--hit:#ff4455;--ok:#25d366}*{box-sizing:border-box}body{margin:0;font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Apple SD Gothic Neo,Noto Sans KR,sans-serif;background:var(--bg);color:var(--ink)}.wrap{max-width:1100px;margin:36px auto;padding:0 16px}.card{background:var(--card);border-radius:20px;padding:20px;box-shadow:0 10px 30px rgba(0,0,0,.35)}h1{margin:0 0 8px}.muted{color:var(--muted);font-size:12px}textarea{width:100%;min-height:160px;padding:14px;border-radius:14px;border:1px solid #263257;background:#0e1430;color:var(--ink);font-size:16px;resize:vertical}button{background:var(--accent);color:white;border:0;padding:12px 16px;border-radius:12px;font-weight:700;cursor:pointer}button:disabled{opacity:.6;cursor:not-allowed}.row{display:flex;gap:12px;flex-wrap:wrap;align-items:center}.grid{margin-top:16px;display:grid;grid-template-columns:1fr 1fr 320px;gap:16px}@media (max-width: 900px) {.grid{grid-template-columns: 1fr;}}.panel{background:#0e1430;border:1px solid #263257;border-radius:14px;padding:14px}mark{background:transparent;color:var(--hit);font-weight:800;text-decoration:underline;text-underline-offset:3px}ins.rep{background:#0f2a1f;color:#b2ffd8;text-decoration:none;border-bottom:2px solid var(--ok);padding:0 2px}.hit{display:flex;justify-content:space-between;gap:8px;border-bottom:1px dashed #263257;padding:8px 0}.pill{font-size:12px;padding:3px 8px;border-radius:999px;background:#1b2342;color:#c7d3ff}</style></head><body>
<div class="wrap">
<h1>ğŸ§­ ì„±ë‚¨ ìƒí™œê¸°ë¡ë¶€ ìë™ ì ê²€ (ë°ëª¨ v1.6)</h1>
<div class="card">
<div class="muted">ë³¸ë¬¸ì„ ë¶™ì—¬ë„£ê³  "ê²€ì‚¬"ë¥¼ ëˆ„ë¥´ì„¸ìš” Â· ì˜¤ë¥¸ìª½ì— <b>ìˆ˜ì •ë³¸ ë¯¸ë¦¬ë³´ê¸°</b>ì™€ <b>ëª¨ë‘ ì ìš©</b>ì´ ìˆì–´ìš”</div>
<textarea id="txt"></textarea>
<div class="row" style="margin-top:8px">
<button id="btn">ê²€ì‚¬</button>
<button id="btnApplyAll" disabled>ëª¨ë‘ ì ìš©</button>
<button id="btnSample">ìƒ˜í”Œ í…ìŠ¤íŠ¸</button>
<span id="lat" class="muted">â€“</span>
<span id="chg" class="muted">ë³€ê²½ 0ê±´</span>
</div>
<div class="grid">
<div class="panel"><div class="muted" style="margin-bottom:8px">í•˜ì´ë¼ì´íŠ¸ ê²°ê³¼(ì›ë¬¸)</div><div id="view" style="line-height:1.8; white-space:pre-wrap;"></div></div>
<div class="panel"><div class="muted" style="margin-bottom:8px">ìˆ˜ì •ë³¸ ë¯¸ë¦¬ë³´ê¸°(ëŒ€ì²´ì–´ ì ìš©)</div><div id="preview" style="line-height:1.8; white-space:pre-wrap;"></div></div>
<div class="panel"><div class="muted" style="margin-bottom:8px">ê·¼ê±° / ëŒ€ì²´í‘œí˜„</div><div id="hits"></div></div>
</div>
</div>
</div>
<script>
const POLICY="2024-03";
const MIN_PREVIEW_CONF = 0.90; // ìë™ ì¹˜í™˜ ê¸°ì¤€
let currentHits = [];
const txtEl = document.getElementById("txt");
function esc(s){return s.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/>/g,"&gt;");}
async function analyze(text){const r=await fetch("/analyze",{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({text:text,policy_version:POLICY})});if(!r.ok)throw new Error(`API Error: ${r.statusText}`);return await r.json();}
// --- Hangul helpers for postposition (ì¡°ì‚¬) auto-fix ---
function _lastHangulHasBatchim(word){
word = (word||"").trim();
if(!word) return false;
const ch = word[word.length-1].charCodeAt(0);
if (ch < 0xAC00 || ch > 0xD7A3) return false;
const jong = (ch - 0xAC00) % 28;
return jong !== 0;
}
function _lastHangulIsRieul(word){
word = (word||"").trim();
if(!word) return false;
const ch = word[word.length-1].charCodeAt(0);
if (ch < 0xAC00 || ch > 0xD7A3) return false;
const jong = (ch - 0xAC00) % 28;
return jong === 8; // ã„¹
}
function chooseParticle(baseWord, particle){
const hasBatchim = _lastHangulHasBatchim(baseWord);
switch(particle){
case "ë¡œ": case "ìœ¼ë¡œ":
if(!hasBatchim || _lastHangulIsRieul(baseWord)) return "ë¡œ";
return "ìœ¼ë¡œ";
case "ì™€": case "ê³¼":
return hasBatchim ? "ê³¼" : "ì™€";
case "ëŠ”": case "ì€":
return hasBatchim ? "ì€" : "ëŠ”";
case "ê°€": case "ì´":
return hasBatchim ? "ì´" : "ê°€";
case "ë¥¼": case "ì„":
return hasBatchim ? "ì„" : "ë¥¼";
default:
return particle;
}
}
// when auto-replacing, adjust the following particle if present
function spliceWithParticle(text, start, end, replacement){
const look = text.slice(end, end+2);
const m = look.match(/^(ìœ¼ë¡œ|ë¡œ|ì„|ë¥¼|ì€|ëŠ”|ì´|ê°€|ê³¼|ì™€)/);
if(m){
const fixed = chooseParticle(replacement, m[0]);
return {
newText: text.slice(0, start) + replacement + fixed + text.slice(end + m[0].length),
skip: m[0].length,
appended: fixed
};
}
return {
newText: text.slice(0, start) + replacement + text.slice(end),
skip: 0,
appended: ""
};
}
function renderResults(text, hits) {
const sortedHits = [...hits].sort((a, b) => a.start - b.start);
let lastIndex = 0;
const viewParts = [];
const previewParts = [];
for (const hit of sortedHits) {
if (hit.start < lastIndex) continue; // overlapped (safety)
if (hit.start > lastIndex) {
const segment = esc(text.slice(lastIndex, hit.start));
viewParts.push(segment);
previewParts.push(segment);
}
// always show original highlight in "ì›ë¬¸"
viewParts.push(`<mark title="${esc(hit.label)}">${esc(hit.span)}</mark>`);
const canReplace = !!hit.replacement && (hit.confidence >= MIN_PREVIEW_CONF);
if (canReplace) {
const look = text.slice(hit.end, hit.end+2);
const m = look.match(/^(ìœ¼ë¡œ|ë¡œ|ì„|ë¥¼|ì€|ëŠ”|ì´|ê°€|ê³¼|ì™€)/);
if(m){
const appended = chooseParticle(hit.replacement, m[0]);
previewParts.push(`<ins class="rep" title="${esc(hit.label)}">${esc(hit.replacement + appended)}</ins>`);
lastIndex = hit.end + m[0].length;
}else{
previewParts.push(`<ins class="rep" title="${esc(hit.label)}">${esc(hit.replacement)}</ins>`);
lastIndex = hit.end;
}
} else {
previewParts.push(`<mark title="${esc(hit.label)}">${esc(hit.span)}</mark>`);
lastIndex = hit.end;
}
}
if (lastIndex < text.length) {
const segment = esc(text.slice(lastIndex));
viewParts.push(segment);
previewParts.push(segment);
}
document.getElementById("view").innerHTML = viewParts.join("").replace(/\\n/g, "<br>");
document.getElementById("preview").innerHTML = previewParts.join("").replace(/\\n/g, "<br>");
const hitsEl = document.getElementById("hits");
hitsEl.innerHTML = "";
if (!hits.length) {
hitsEl.innerHTML = '<div class="muted">ê·œì • ìœ„ë°˜ í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ğŸ‘Œ</div>';
return;
}
for (const h of hits) {
const row = document.createElement("div");
row.className = "hit";
const auto = (h.replacement && h.confidence >= MIN_PREVIEW_CONF);
const conf = Math.round(h.confidence * 100);
row.innerHTML =
`<div>
<b>${esc(h.span)}</b> <span class="pill">${h.label}</span> <span class="pill">${conf}%</span><br/>
<span class="muted">${h.source.doc} p.${h.source.page||'?'}: ${esc(h.source.quote||'')}</span>
</div>` +
(h.replacement ? `<div class="pill">${auto ? 'ìë™ì ìš©' : 'ê²€í† í•„ìš”'}: ${esc(h.replacement)}</div>` : "");
hitsEl.appendChild(row);
}
}
function applyAllReplacements() {
let text = txtEl.value;
const replacableHits = currentHits
.filter(h => h.replacement && h.confidence >= MIN_PREVIEW_CONF)
.sort((a,b) => b.start - a.start);
for (const hit of replacableHits) {
const sp = spliceWithParticle(text, hit.start, hit.end, hit.replacement);
text = sp.newText;
}
txtEl.value = text;
document.getElementById("btn").click();
}
document.getElementById("btnSample").onclick = function() {
txtEl.value = 'ìœ ì—”(UN) ë³´ê³ ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ì±—GPT ì´ˆì•ˆ ì‘ì„± í›„ MSì›Œë“œ ì •ë¦¬í•˜ê³  Google Docsì— ì˜®ê²¼ë‹¤.\\nZoom(ì›¨ì¼ì˜¨)ìœ¼ë¡œ ë°œí‘œí•˜ê³  yutubeÂ·Instagramì— í™ë³´í–ˆë‹¤.\\nì´ë™ì€ KTX, í‘œì§€ëŠ” Canva ì œì‘, í¸ì§‘ì€ í‚¤ë„¤ë§ˆìŠ¤í„° ë§ˆë¬´ë¦¬í–ˆìœ¼ë©° ì†Œë…¼ë¬¸ë„ ì œì¶œí–ˆë‹¤. ë˜í•œ Jupyter í†µí•´ ì‹¤í—˜ì„ ì •ë¦¬í–ˆê³  CRISPR-Cas9 ê´€ë ¨ ë‚´ìš©ì„ ì°¸ê³ í–ˆë‹¤. Java Script ë° JSë„ ì‚¬ìš©í–ˆë‹¤.';
};
document.getElementById("btn").onclick = async function() {
const text = txtEl.value || "";
this.textContent = "ê²€ì‚¬ ì¤‘...";
this.disabled = true;
try {
const res = await analyze(text);
currentHits = res.hits;
document.getElementById("lat").textContent = `ì²˜ë¦¬ì‹œê°„: ${res.latency_ms} ms`;
renderResults(text, res.hits);
const changedCount = currentHits.filter(h => h.replacement && h.confidence >= MIN_PREVIEW_CONF).length;
document.getElementById("chg").textContent = `ë³€ê²½ ${changedCount}ê±´`;
const btnApply = document.getElementById("btnApplyAll");
btnApply.disabled = changedCount === 0;
btnApply.onclick = applyAllReplacements;
} catch (e) {
alert("ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\\n(Hugging Face ì»¨í…Œì´ë„ˆ ê¸°ìƒ ì¤‘ì¼ ìˆ˜ ìˆìŒ)");
console.error(e);
} finally {
this.textContent = "ê²€ì‚¬";
this.disabled = false;
}
};
document.getElementById("btnSample").click();
</script></body></html>
"""

# =========================
# API Routes
# =========================
@app.get("/", response_class=HTMLResponse, summary="Main UI Page")
def home():
    return HTML_PAGE


@app.post("/analyze", response_model=AnalyzeResponse, summary="Analyze student record text")
def analyze(payload: AnalyzeRequest = Body(...)):
    t0 = time.perf_counter()
    # 1) Regex Analysis Unit
    hits_rule = regex_match(payload.text)
    # 2) Exact Alias Unit (safe auto-fix for common typos)
    hits_alias = alias_exact_match(payload.text)
    # 3) Collapse parenthetical duplicates like 'ìœ ì—”(UN)'
    primary_hits = collapse_parenthetical_duplicates(payload.text, hits_rule + hits_alias)
    # 4) Embedding Analysis Unit (conservative)
    hits_semantic = semantic_match(payload.text)
    # 5) Merge & respond
    final_hits = merge_hits(primary_hits, hits_semantic)
    latency_ms = int((time.perf_counter() - t0) * 1000)
    return AnalyzeResponse(hits=final_hits, latency_ms=latency_ms)


